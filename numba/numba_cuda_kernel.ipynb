{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"numba_cuda_kernel.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fm8wdYqVBeXj","colab_type":"text"},"source":["## Numba + CUDA on Google Colab\n","\n","By default, Google Colab is not able to run numba + CUDA, because two libraries are not found, `libdevice` and `libnvvm.so`. So we need to make sure that these libraries are found in the notebook.  \n","\n","First, we look for these libraries on the system. To do that, we simply run the `find` command, to recursively look for these libraries starting at the root of the filesystem. The exclamation mark escapes the line so that it's executed by the Linux shell, and not by the jupyter notebook. "]},{"cell_type":"code","metadata":{"id":"5Mt7dgLwmnVJ","colab_type":"code","outputId":"730e90a1-605e-412d-d950-66c34a1dbd96","executionInfo":{"status":"ok","timestamp":1565711568775,"user_tz":-120,"elapsed":3900,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!find / -iname 'libdevice'\n","!find / -iname 'libnvvm.so'\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/cuda-10.0/nvvm/libdevice\n","/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HO0mdhPoDRTS","colab_type":"text"},"source":["Then, we add the two libraries to numba environment variables:"]},{"cell_type":"code","metadata":{"id":"Ctr6aM3cmkdx","colab_type":"code","colab":{}},"source":["import os\n","os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.0/nvvm/libdevice\"\n","os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whscBGwEDcEZ","colab_type":"text"},"source":["And we're done! "]},{"cell_type":"markdown","metadata":{"id":"iah5fiDRtVYt","colab_type":"text"},"source":["## A very simple CUDA kernel\n","\n","Let's get started by implementing a first CUDA kernel to compute the square root of each value in an array. First, here is our array: "]},{"cell_type":"code","metadata":{"id":"RGzqgz7DuK9n","colab_type":"code","outputId":"bd8a0cf4-2b3f-44d7-80d0-645bb00e87af","executionInfo":{"status":"ok","timestamp":1565711568776,"user_tz":-120,"elapsed":1554,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import numpy as np\n","a = np.arange(4096,dtype=np.float32)\n","a"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.000e+00, 1.000e+00, 2.000e+00, ..., 4.093e+03, 4.094e+03,\n","       4.095e+03], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"DroQCdZpuU5E","colab_type":"text"},"source":["As we have seen in [part II](https://thedatafrog.com/boost-python-gpu/), and as discussed in the introduction, we can simply use numba's vectorize decorator to compute the square root of all elements in parallel on the GPU: "]},{"cell_type":"code","metadata":{"id":"Dhmy-LmjuRCX","colab_type":"code","outputId":"f22fe71b-6ce2-442d-f312-1851520e2222","executionInfo":{"status":"ok","timestamp":1565711569469,"user_tz":-120,"elapsed":1250,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import math\n","from numba import vectorize\n","\n","@vectorize(['float32(float32)'], target='cuda')\n","def gpu_sqrt(x):\n","    return math.sqrt(x)\n","  \n","gpu_sqrt(a)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n","       63.992188 ], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"i1LUuMmBu2sB","colab_type":"text"},"source":["This time, as an exercise, we'll do the same with a custom CUDA kernel. \n","\n","We first define our kernel: "]},{"cell_type":"code","metadata":{"id":"ln2DfOpjuvfp","colab_type":"code","colab":{}},"source":["from numba import cuda\n","\n","@cuda.jit\n","def gpu_sqrt_kernel(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.sqrt(x[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ruRzBxXgwZyh","colab_type":"text"},"source":["Let's discuss this code in some details. \n","\n","We have an input array of 4096 values, so we will use 4096 threads on the GPU. \n","\n","Our input and output arrays are one dimensional, so we will use a one-dimensional *grid* of threads (we will discuss grids in details in the next section). The call `cuda.grid(1)` returns the unique index for the current thread in the whole grid.  With 4096 threads, `idx` will range from 0 to 4095. \n","\n","Then, we see in the code that each thread is going to deal with a single element of the input array to produce a single element in the output array. This element is determined for each thread by the thread index. \n","\n","Now that we have our kernel, we copy our input array to the GPU device, create an output array on the device with the same shape, and finally launch the kernel: "]},{"cell_type":"code","metadata":{"id":"cN1flx7Dx_nT","colab_type":"code","outputId":"59dcbcde-e105-4ada-f6ea-f8fced935bcf","executionInfo":{"status":"ok","timestamp":1565711576506,"user_tz":-120,"elapsed":378,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# move input data to the device\n","d_a = cuda.to_device(a)\n","# create output data on the device\n","d_out = cuda.device_array_like(d_a)\n","\n","# we decide to use 32 blocks, each containing 128 threads\n","blocks_per_grid = 32\n","threads_per_block = 128\n","gpu_sqrt_kernel[blocks_per_grid, threads_per_block](d_a, d_out)\n","# wait for all threads to complete\n","cuda.synchronize()\n","# copy the output array back to the host system\n","# and print it\n","print(d_out.copy_to_host())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[ 0.         1.         1.4142135 ... 63.97656   63.98437   63.992188 ]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TL3-d3S0z_RM","colab_type":"text"},"source":["In the code above, the 4096 threads are arranged into a grid of 32 *blocks*, each block having 128 threads. This *execution configuration* will be discussed in the next section. \n","\n","**Exercises:**\n","\n","- Go back to the previous cell, and try to decrease the number of blocks per grid, or the number of threads per block. \n","- Then try to increase the number of blocks per grid, or the number of threads per blocks\n","- Try to remove the `cuda.synchronize()` call\n","\n","**Results**: \n","\n","- When you reduce the number of threads, either by decreasing the number of blocks per grid or the number of threads per block, some elements are not processed, and the corresponding slots at the end of the output array remain set to their default value, which is 0. \n","- If, on the other hand, you increase the number of threads, it seems that everything is working fine. However, this actually creates an error even though we cannot see it. We will see later how to expose this error. Debugging code is one of the difficulties of CUDA, as the error messages are not always visible.\n","- Finally, you might have expected that commenting out the call to `cuda.synchronize` would have resulted in output array only partially filled, or not filled at all. That would be a good guess since the CPU keeps running the main program (the one of the cell) while the GPU processes the data asynchronously. However, the copy to the host performs an implicit synchronization, so the call to cuda.syncronize is not necessary.  "]},{"cell_type":"markdown","metadata":{"id":"Ces_Ss67GqzR","colab_type":"text"},"source":["## Execution configuration : number of blocks and number of threads\n","\n","In our first example above, we decided to use 4096 threads. These threads were arranged as a grid containing 32 blocks of 128 threads each. \n","\n","You probably wondered why we decided to do that. So in this section, I will try and answer some of the questions you may have: \n","\n","- Why do we need to arrange threads into blocks within the grid? \n","- How did we come up with the magical numbers 32 and 128? Could we use other values like 16 and 256 (which still total 4096), or 64 and 64? \n","\n","These questions are important if you care about performance -- and you probably do if you're here to learn how to speed up your calculations on a GPU! To answer these questions, we first need to learn about our GPU hardware. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"dAH9lfmuIIRF","colab_type":"text"},"source":["### CUDA: Learn about your GPU hardware\n","\n","Let's find out about the GPU we are using. Please note that your GPU (or the GPU attributed to you on Google Colab) could be different from mine. Of course, the numbers I'm giving below and the picture are only valid for the GPU attributed to me during my session on Google Colab. "]},{"cell_type":"code","metadata":{"id":"fACSmHLzJanZ","colab_type":"code","outputId":"90bfffb9-3638-4162-f9e1-d84e7ed0cd2c","executionInfo":{"status":"ok","timestamp":1565711583203,"user_tz":-120,"elapsed":480,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["cuda.detect()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Found 1 CUDA devices\n","id 0             b'Tesla T4'                              [SUPPORTED]\n","                      compute capability: 7.5\n","                           pci device id: 4\n","                              pci bus id: 0\n","Summary:\n","\t1/1 devices are supported\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"BM_fu2ZvM5dJ","colab_type":"text"},"source":["We see that's an nvidia [T4](https://www.nvidia.com/en-us/data-center/tesla-t4/), and here is the [white paper with the specs](https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf) for the Turing architecture. In this paper, we see that the T4 is based on the Turing TU102 GPU which has: \n","\n","- 72 *streaming multiprocessors* \n","- 64 cuda cores per streaming multiprocessor\n","\n","Here is a block diagram of the TU102 GPU. The streaming multiprocessors are the 72 inner blocks labelled SM.\n","\n","![TU102 GPU](https://raw.githubusercontent.com/cbernet/maldives/master/numba/tu_102_diagram.jpg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G75hEplWqIUW","colab_type":"text"},"source":["The [streaming multiprocessors](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) are the fundamental processing units of CUDA enabled GPUs. Each SM can execute hundreds of threads in parallel, and has all the resources needed to manage these threads. "]},{"cell_type":"markdown","metadata":{"id":"0SpW53BJyZgh","colab_type":"text"},"source":["### Number of blocks and number of threads per block\n","\n","When the kernel is launched, each block of threads is sent to a single SM, and each SM can end up with multiple blocks to process. \n","\n","The first conclusion we can draw is that if the number of blocks is lower than the number of SMs in the GPU, some of the SMs will be idle. That was the case in our first simple example above: we used 32 blocks, and we have 72 SMs on the Tesla T4. Therefore, 40 SMs remained unused. \n","\n","To process a block, the SM partitions it into groups of 32 threads called *warps*. A warp executes one common instruction at a given time in parallel for all threads in the warp. So full efficiency is realized if all warps in the block are complete. Therefore, the number of threads per block needs to be a multiple of 32. Moreover, to limit latency, this number should not be too low. \n","\n","Deciding which execution configuration to use is not easy, and the choice should be driven by performance analysis. However, here are some basic rules to get started: \n","\n","- **The number of blocks in the grid should be larger than the number of SMs on the GPU, typically 2 to 4 times larger.** \n","- **The number of threads per block should be a multiple of 32, typically between 128 and 512.**\n","\n","The Tesla T4 has 72 streaming multiprocessors. On paper, it will perform best on data arrays with a size larger than 72 * 2 * 128 ~ 20 000. And this is the bare minimum: this card will typically be used on arrays much larger than that. In our simple example above, we used an array of size 4096, which is way too small.\n","\n","On the other hand, what if the data array is very large? Let's consider an array with one billion entries. Following the second rule above, we can choose a number of threads per block of 256, and we end up with 1e9 / 256 ~ 3 million blocks. Since the CUDA kernel launch overhead increases with the number of blocks, going for such a large number of blocks would hit performance. In the following, we will see how to use striding to solve this problem. "]},{"cell_type":"markdown","metadata":{"id":"rGtoDVrUAFR5","colab_type":"text"},"source":["### Striding \n","\n","This simple kernel deals with a single element of the input array:"]},{"cell_type":"code","metadata":{"id":"PqAPolFrWg8_","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.atan(x[idx])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k3F70nyMW1L3","colab_type":"text"},"source":["When the kernel is deployed, the GPU therefore needs to create as many threads as elements in the array, which potentially results in many blocks if the array is large. \n","\n","On the contrary, a striding kernel deals with several elements of the input array, using a loop: "]},{"cell_type":"code","metadata":{"id":"NXP0I0Tk9QNp","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan_stride(x, out):\n","  start = cuda.grid(1)\n","  stride = cuda.gridsize(1)\n","  for i in range(start, x.shape[0], stride): \n","    out[i] = math.atan(x[i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bnIMk8A8Xw_R","colab_type":"text"},"source":["In this way, a given thread deals with several elements, and the number of threads is kept under control. Threads keep doing work in a coordinated way, and the GPU is not wasting time creating and scheduling threads. \n","\n","Let's consider a small example with: \n","\n","- an input data array of size 8\n","- blocks with 4 threads each\n","\n","Without striding, we need to use two blocks so 8 threads in total, each dealing with a single element in the array: \n"]},{"cell_type":"code","metadata":{"id":"szWBj0wQzxT8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"7041ad4e-6c0b-4bfb-e542-6ecb9ea7a3ef","executionInfo":{"status":"ok","timestamp":1565713717125,"user_tz":-120,"elapsed":579,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["a = np.arange(8, dtype=np.float32)\n","d_a = cuda.to_device(a)\n","d_out = cuda.device_array_like(d_a)\n","\n","# 2 blocks of 4 threads\n","gpu_atan[2,4](d_a, d_out)\n","print(d_out.copy_to_host())"],"execution_count":53,"outputs":[{"output_type":"stream","text":["[0.        0.7853982 1.1071488 1.2490458 1.3258177 1.3734008 1.4056478\n"," 1.4288993]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-jvM3p8SzrwH","colab_type":"text"},"source":["With striding, we can use a single block with 4 threads, in which case each thread deals with two elements.  Specifically, in the code above: \n","\n","- `start` is the thread index, which is 0, 1, 2, 3 for the four threads, respectively. \n","- `stride` is the size of the grid, which is the total number of threads in the grid, here 4. \n","- `x.shape[0]` is the size of the input data array `x`, which is 8. \n"]},{"cell_type":"code","metadata":{"id":"NI7Jne23zKE9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"be99fb4b-0fae-4ed4-a6c7-3ab9251822a9","executionInfo":{"status":"ok","timestamp":1565713956846,"user_tz":-120,"elapsed":397,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["# 1 block of 4 threads\n","gpu_atan_stride[1,4](d_a, d_out)\n","print(d_out.copy_to_host())"],"execution_count":55,"outputs":[{"output_type":"stream","text":["[0.        0.7853982 1.1071488 1.2490458 1.3258177 1.3734008 1.4056478\n"," 1.4288993]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uWxdTs1IzFdV","colab_type":"text"},"source":["Here are the elements processed by each thread:"]},{"cell_type":"code","metadata":{"id":"I6dK_HEvaJB3","colab_type":"code","outputId":"fafaa200-0661-451b-f70b-d666b357076e","executionInfo":{"status":"ok","timestamp":1565711649375,"user_tz":-120,"elapsed":555,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["for thread_index in range(4):\n","  print('thread', thread_index)\n","  for j in range(thread_index, 8, 4):\n","    print('\\telement', j)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["thread 0\n","\telement 0\n","\telement 4\n","thread 1\n","\telement 1\n","\telement 5\n","thread 2\n","\telement 2\n","\telement 6\n","thread 3\n","\telement 3\n","\telement 7\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xd6YJF1Efdls","colab_type":"text"},"source":["A useful way to think about this is to imagine that the grid is moving to process all elements in the input array, as shown below. In this picture, each color corresponds to a thread (e.g. thread 0 processes elements 0 and 4). \n","\n","Of course, even though this representation can help you figure out how to code your striding, keep in mind that threads still proceed asynchronously. In other words, thread 0 might be done with elements 0 and 4 while thread 1 is still dealing with element 1.\n","\n","![](https://raw.githubusercontent.com/cbernet/maldives/master/numba/striding.png)"]},{"cell_type":"markdown","metadata":{"id":"BIgZRnm6Jku2","colab_type":"text"},"source":["### Performance analysis\n","\n","In this section, we will study the influence of striding and of the execution configuration parameters in the processing of a large array. \n","\n","\n","So let's redefine our kernels (the simple one and the striding one:) "]},{"cell_type":"code","metadata":{"id":"Ilce8lN07ZEl","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan(x, out):\n","  idx = cuda.grid(1)\n","  out[idx] = math.atan(x[idx])\n","  \n","@cuda.jit\n","def gpu_atan_stride(x, out):\n","  start = cuda.grid(1)\n","  stride = cuda.gridsize(1)\n","  for i in range(start, x.shape[0], stride): \n","    out[i] = math.atan(x[i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tzx1NQzxhAHD","colab_type":"text"},"source":["Now we create a big array, we ship it to the device, and we create the output array on the device as usual: "]},{"cell_type":"code","metadata":{"id":"7fPetfTF5NiY","colab_type":"code","colab":{}},"source":["import numpy as np\n","a = np.arange(256*1000000,dtype=np.float32)\n","# move input data to the device\n","d_a = cuda.to_device(a)\n","# create output data on the device\n","d_out = cuda.device_array_like(d_a)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7Ovofmo51EC","colab_type":"text"},"source":["First, let's see how fast we can process this array sequentially (in a single thread) on the GPU. We use the striding version here since, obviously, the simple version would only be able to process one element in a single thread. "]},{"cell_type":"code","metadata":{"id":"YM2Q8ys3LKaI","colab_type":"code","outputId":"4cdab270-461b-4ead-81fb-aad683ecc407","executionInfo":{"status":"ok","timestamp":1565714121214,"user_tz":-120,"elapsed":55223,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# [1,1] means : 1 block with 1 thread\n","%time gpu_atan_stride[1, 1](d_a, d_out); cuda.synchronize()"],"execution_count":58,"outputs":[{"output_type":"stream","text":["CPU times: user 29.9 s, sys: 24.9 s, total: 54.8 s\n","Wall time: 54.8 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U4QMhYa9hSPJ","colab_type":"text"},"source":["Processing these 256 million values in a single thread on the GPU took almost a minute. Let's see how parallel processing can help us. For that, we choose an execution configuration by following our simple rules, and we use the non-striding kernel:"]},{"cell_type":"code","metadata":{"id":"VoDEtjKAxywM","colab_type":"code","outputId":"ad6b322d-1e28-4ccc-a81d-f18c045b012d","executionInfo":{"status":"ok","timestamp":1565714171716,"user_tz":-120,"elapsed":400,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# RUN THIS TWICE. \n","# sometimes, the first time, the timing is way too large for some reason, \n","# and not representative of the actual timing\n","# you should get something around 13 ms. \n","%time gpu_atan[1000000,256](d_a, d_out); cuda.synchronize()"],"execution_count":61,"outputs":[{"output_type":"stream","text":["CPU times: user 6.62 ms, sys: 3.47 ms, total: 10.1 ms\n","Wall time: 12.5 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cC8Qey0kh_vG","colab_type":"text"},"source":["The parallel version is much faster as expected. \n","\n","Now, let's try and see if the striding version brings any performance improvement:  "]},{"cell_type":"code","metadata":{"id":"JNhYXML25Uuj","colab_type":"code","outputId":"32727fef-be1a-4e01-fafe-614a86daf59d","executionInfo":{"status":"ok","timestamp":1565714203053,"user_tz":-120,"elapsed":407,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# RUN THIS TWICE\n","%time gpu_atan_stride[50000,256](d_a, d_out); cuda.synchronize()"],"execution_count":63,"outputs":[{"output_type":"stream","text":["CPU times: user 6.06 ms, sys: 2.04 ms, total: 8.11 ms\n","Wall time: 10.7 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WSAoEAvCs9i5","colab_type":"text"},"source":["The gain is not very significant, you might have to rerun the two previous cells several times to convince yourself that there is indeed a gain. The amount of performance gain you'll get from striding will depend on the size of the input array, and on the work to be done by the kernel. \n","\n","**Exercise:**\n","\n","Try and change the number of blocks in the cell just above and re-run several times for each value to see how this parameter affects the timing. Try e.g. 100, 1 000, 100 000, and 500 000. Then, take the best value you could find for the number of blocks, and try to find the best value for the number of threads per block in the same way. "]},{"cell_type":"markdown","metadata":{"id":"GVl1tELFDvP8","colab_type":"text"},"source":["## Multidimensional datasets : Matrix multiplication\n","\n","So far, we've been working with one-dimensional arrays, making use of a 1D grid of threads. \n","\n","In the following 1D kernel, `cuda.grid(1)` returns a single index that identifies the position of the thread in the grid, and and `cuda.gridsize(1)` returns the length of the grid of threads:"]},{"cell_type":"code","metadata":{"id":"RyCdIpuOB27q","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_atan_stride(x, out):\n","  start = cuda.grid(1)\n","  stride = cuda.gridsize(1)\n","  for i in range(start, x.shape[0], stride): \n","    out[i] = math.atan(x[i])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5rIhdOYnE33B","colab_type":"text"},"source":["As we will see, these functions also provide an easy interface for the processing of 2D and 3D data. \n","\n","In this  section, you will learn how to deploy a 2D grid of threads to process 2D data, and how to stride through 2D data. Then, you will implement a practical case: matrix multiplication. "]},{"cell_type":"markdown","metadata":{"id":"p_m6Cwr5F4O1","colab_type":"text"},"source":["### A simple 2D kernel \n","\n","To learn the basic tools to work with 2D data, please consider the following kernel: "]},{"cell_type":"code","metadata":{"id":"N0vwJ_3jF3j7","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_2d(out): \n","  # get the thread coordinates in 2D\n","  i1, i2 = cuda.grid(2)\n","  out[i1][i2] = i1*10 + i2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sDyp9wM4GyKC","colab_type":"text"},"source":["As you can see, this kernel does not even have an input. Its goal is only to encode and record in the output 2D array the coordinates of the current thread. \n","\n","Now, let's create the output data structure: "]},{"cell_type":"code","metadata":{"id":"qkqRBjwpHMKb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"6dd0d366-abf7-40d9-8976-54c826d45b08","executionInfo":{"status":"ok","timestamp":1565714328427,"user_tz":-120,"elapsed":393,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["a = np.zeros(12).reshape(3,4)\n","a\n"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"Uup3q48UHbRn","colab_type":"text"},"source":["We copy this data structure to the device, and we deploy the kernel, before fetching the results: "]},{"cell_type":"code","metadata":{"id":"42AjVNQ5HYCH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"38d63e53-5b62-4939-8693-0175c2b50677","executionInfo":{"status":"ok","timestamp":1565714383657,"user_tz":-120,"elapsed":412,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["d_a = cuda.to_device(a)\n","# we use two blocks, side-by-side in the horizontal direction\n","blocks = (1,2)\n","# each block has 6 threads arranged in 3 lines and 2 columns\n","threads_per_block = (3,2)\n","gpu_2d[blocks, threads_per_block](d_a)\n","d_a.copy_to_host()"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.,  1.,  2.,  3.],\n","       [10., 11., 12., 13.],\n","       [20., 21., 22., 23.]])"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"TDUbxWu9IW5Y","colab_type":"text"},"source":["The first two columns were processed by the first block of threads, and the last two by the second block.\n","\n","Looking at the kernel code and at the output above, we see that `i1` indexes the first dimension (the lines) while `i2` indexes the second dimension (the columns).\n","\n","As in the 1D case, it's necessary to make sure that the grid covers the whole output data structure. For example, if we mess up in the description of the block structure, the last two columns remain unprocessed, and we actually access unallowed memory \"below\" our 2D array: \n","\n"]},{"cell_type":"code","metadata":{"id":"ekqs9OKhH7oh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"4a7cf6fc-14fb-40a8-b09a-d43326610137","executionInfo":{"status":"ok","timestamp":1565714430393,"user_tz":-120,"elapsed":389,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["# need to send back the matrix of zeros \n","# to reset it on the device\n","d_a = cuda.to_device(a)\n","# here we mess up: \n","# we require two blocks on top of each other, \n","# and we get a grid with a total height of 2x3=6 \n","# and a total width of 1x2=2...\n","gpu_2d[(2,1), (3,2)](d_a)\n","d_a.copy_to_host()"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.,  1.,  0.,  0.],\n","       [10., 11.,  0.,  0.],\n","       [20., 21.,  0.,  0.]])"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"AmZnCcMYKphT","colab_type":"text"},"source":["### 2D striding \n","\n","Striding in 2D is not much more difficult that in 1D. Again, let's take a very simple example, \n","and first make sure that our implementation of striding is working: "]},{"cell_type":"code","metadata":{"id":"3NRXU31AJUqX","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_2d_stride(out): \n","  # get the thread coordinates in 2D\n","  s1, s2 = cuda.grid(2)\n","  # get the grid size in 2D\n","  d1, d2 = cuda.gridsize(2)\n","  for i1 in range(s1, out.shape[0], d1):\n","    for i2 in range(s2, out.shape[1], d2): \n","      out[i1][i2] = i1*10 + i2\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m0Pp47EwMLwT","colab_type":"text"},"source":["We reset the output data on the device, and we deploy a **single** `(3,2)` block. The grid therefore covers only half of the output array. But the striding will move the grid in the output array, making sure that the whole array is covered: "]},{"cell_type":"code","metadata":{"id":"LNwcRAJoLgLv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ebe43c4b-4abe-4e15-dcd8-8ef33dcda5d1","executionInfo":{"status":"ok","timestamp":1565714457016,"user_tz":-120,"elapsed":627,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["d_a = cuda.to_device(a)\n","gpu_2d_stride[(1,), (3,2)](d_a)\n","d_a.copy_to_host()"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.,  1.,  2.,  3.],\n","       [10., 11., 12., 13.],\n","       [20., 21., 22., 23.]])"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"markdown","metadata":{"id":"WBCwEdV2L5tR","colab_type":"text"},"source":["We get the same results as without striding. Now let's visualize striding with a slightly different kernel, which will register the starting point `(s1, s2)` of each thread: "]},{"cell_type":"code","metadata":{"id":"Y8Jrqf-WLp5h","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def gpu_2d_start(out): \n","  # get the thread coordinates in 2D\n","  s1, s2 = cuda.grid(2)\n","  # get the grid size in 2D\n","  d1, d2 = cuda.gridsize(2)\n","  for i1 in range(s1, out.shape[0], d1):\n","    for i2 in range(s2, out.shape[1], d2):\n","      # note the difference w/r to the previous kernel\n","      out[i1][i2] = s1*10 + s2\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYl3zb8uNH0D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"4dd64be6-c4ed-4789-f610-37dede019d66","executionInfo":{"status":"ok","timestamp":1565714466893,"user_tz":-120,"elapsed":526,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["d_a = cuda.to_device(a)\n","gpu_2d_start[(1,), (3,2)](d_a)\n","d_a.copy_to_host()"],"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.,  1.,  0.,  1.],\n","       [10., 11., 10., 11.],\n","       [20., 21., 20., 21.]])"]},"metadata":{"tags":[]},"execution_count":73}]},{"cell_type":"markdown","metadata":{"id":"NMXlZUhANQB-","colab_type":"text"},"source":["The elements with the same value are processed by the same thread, and we see that there is only one stride to the right. \n","\n","Extending our 2D array, we see that striding occurs in both direction: "]},{"cell_type":"code","metadata":{"id":"4mmvUs4GN83y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"e0e2c21c-5dee-4d3a-89f3-d7cbc4e3358a","executionInfo":{"status":"ok","timestamp":1565714491970,"user_tz":-120,"elapsed":401,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["a = np.zeros(16).reshape(4,4)\n","d_a = cuda.to_device(a)\n","gpu_2d_start[(1,), (3,2)](d_a)\n","d_a.copy_to_host()"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.,  1.,  0.,  1.],\n","       [10., 11., 10., 11.],\n","       [20., 21., 20., 21.],\n","       [ 0.,  1.,  0.,  1.]])"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"markdown","metadata":{"id":"REXr9s_UOMRh","colab_type":"text"},"source":["Here we have four strides arranged in a `(2,2)` structure. The strides at the bottom are incomplete, but the kernel is naturally protected against overflow since the loops are limited to the dimensions of the output array. \n","\n","**Exercise:** \n","\n","We see that thread `(0,0)` processes four elements of the output array just above. Look at the kernel code and find out in which order the thread processes these four elements. "]},{"cell_type":"markdown","metadata":{"id":"iDahbF0NNlPh","colab_type":"text"},"source":["### 2D matrix multiplication : theory\n","\n","Implement a 2D matrix multiplication kernel is an excellent way to confirm that we now master striding in 2D. \n","\n","First, a little reminder. Two matrices A of size (m,n) and B of size (n,p) can be multiplied since the number of colums of matrix A is equal to the number of lines of matrix B. The product matrix C is then of size (m,p). And the value of one of the coefficients of matrix C is given by: \n","\n","$$c_{ij} = \\sum_{k=0}^{n} a_{ik} b_{kj}$$\n","\n","This might seem a bit complicated if you are new to linear algebra or haven't used it in a long time. So let's take a simple example in code. We create two matrices: \n"]},{"cell_type":"code","metadata":{"id":"lL3MIa8ENJkJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"29d8ba91-3a91-43bd-a22a-b30e55e04719","executionInfo":{"status":"ok","timestamp":1565714497852,"user_tz":-120,"elapsed":373,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["a = np.arange(6).reshape(2,3)\n","b = np.arange(12).reshape(3,4)\n","print(a)\n","print(b)"],"execution_count":75,"outputs":[{"output_type":"stream","text":["[[0 1 2]\n"," [3 4 5]]\n","[[ 0  1  2  3]\n"," [ 4  5  6  7]\n"," [ 8  9 10 11]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"q4fdJxRBU0Ji","colab_type":"text"},"source":["And we do the multiplication with numpy (on the CPU): "]},{"cell_type":"code","metadata":{"id":"bhh8674RP9W0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"702af068-743e-45ba-88fe-676c49675841","executionInfo":{"status":"ok","timestamp":1565714501030,"user_tz":-120,"elapsed":383,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["c = a.dot(b)\n","c"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[20, 23, 26, 29],\n","       [56, 68, 80, 92]])"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"markdown","metadata":{"id":"xt2t8adjU2oG","colab_type":"text"},"source":["Matrix `a` is of size `(2,3)` and matrix `b` of size `(3,4)`, so the product matrix is of size `(2,4)`. \n","\n","Let's consider for example the coefficient on the first line and second column of the product matrix, which is 23. This is c01. The summation equation above tells us that this coefficient is obtained by taking the coefficients of the first line of matrix a, and by multiplying them with the coefficients of the second column of matrix b, respectively, before summing everything. \n","\n","We get: `c01 = a00*b01 + a01*b11 + a02*b21 = 0*1 + 1*5 +  2*9 = 23`"]},{"cell_type":"markdown","metadata":{"id":"Fe1ii4j6XCi2","colab_type":"text"},"source":["### A 2D matrix multiplication kernel\n","\n","As a first step, we will implement matrix multiplication without striding. So we will need one thread for each element in the output matrix. Here is the kernel: "]},{"cell_type":"code","metadata":{"id":"9F6o1JjQQJF1","colab_type":"code","colab":{},"cellView":"both"},"source":["@cuda.jit\n","def multiply(a, b, c): \n","  i1, i2 = cuda.grid(2)\n","  the_sum = 0\n","  for k in range(b.shape[0]): # or a.shape[1] since they are equal\n","    the_sum += a[i1][k]*b[k][i2]\n","  c[i1, i2] = the_sum"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"211zfwsUZRRi","colab_type":"text"},"source":["We ship the matrices `a` and `b` to the device, and create the output matrix `c`: "]},{"cell_type":"code","metadata":{"id":"nBf-tftCZMBa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1da3f0ca-6807-4f2d-dbf1-71bb3ecc8c7d","executionInfo":{"status":"ok","timestamp":1565714528528,"user_tz":-120,"elapsed":394,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["d_a = cuda.to_device(a)\n","d_b = cuda.to_device(b)\n","c = np.zeros((a.shape[0], b.shape[1]))\n","print(c.shape)\n","d_c = cuda.to_device(c)"],"execution_count":78,"outputs":[{"output_type":"stream","text":["(2, 4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TUJKuT8mwWwt","colab_type":"text"},"source":["Then, we deploy the kernel. We use a single block with 8 threads, arranged as a 2D array with 2 lines and 4 columns, matching the shape of the output matrix: "]},{"cell_type":"code","metadata":{"id":"xDfVfuZhZze4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"7440e90d-6168-4ae7-b82f-95b24671b524","executionInfo":{"status":"ok","timestamp":1565714538911,"user_tz":-120,"elapsed":542,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["multiply[(1,), (2,4)](d_a, d_b, d_c)\n","print(d_c.copy_to_host())"],"execution_count":79,"outputs":[{"output_type":"stream","text":["[[20. 23. 26. 29.]\n"," [56. 68. 80. 92.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SEBcGAUBbk83","colab_type":"text"},"source":["Fine, we get the same results as with numpy. \n","\n","Implementing striding is really no big deal. The only thing that could be a bit tricky is to remember that we need to stride on the output matrix, which has shape `(a.shape[0], b.shape[1])`. For striding, the inputs are irrelevant! "]},{"cell_type":"code","metadata":{"id":"grNQUrHsaEOR","colab_type":"code","colab":{}},"source":["@cuda.jit\n","def multiply_stride(a, b, c): \n","  s1, s2 = cuda.grid(2)\n","  d1, d2 = cuda.gridsize(2)\n","  for i1 in range(s1, a.shape[0], d1): \n","    for i2 in range(s2, b.shape[1], d2): \n","      the_sum = 0\n","      for k in range(b.shape[0]): # or a.shape[1] \n","        the_sum += a[i1][k]*b[k][i2]\n","      c[i1, i2] = the_sum"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wdHbNk7VbE4H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"a9168ad5-4254-4732-c0b7-8eb34e182d59","executionInfo":{"status":"ok","timestamp":1565714578867,"user_tz":-120,"elapsed":693,"user":{"displayName":"Colin Bernet","photoUrl":"https://lh5.googleusercontent.com/-PT0Y40VvMq0/AAAAAAAAAAI/AAAAAAAAFEI/VsJO93FEElA/s64/photo.jpg","userId":"10813031011844134122"}}},"source":["# shipping back c to reset it on the device: \n","d_c = cuda.to_device(c)\n","# using a single block of shape (2,2) \n","# so we have 4 threads and each one produces\n","# two elements in the output matrix. \n","# in other words, the grid of (2,2) threads is moved \n","# once to the right in the output matrix. \n","multiply_stride[(1,), (2,2)](d_a, d_b, d_c)\n","print(d_c.copy_to_host())"],"execution_count":81,"outputs":[{"output_type":"stream","text":["[[20. 23. 26. 29.]\n"," [56. 68. 80. 92.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yiXw8rRr3Hqv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}